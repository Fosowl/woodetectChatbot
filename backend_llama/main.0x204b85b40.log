[1697897870] Log start
[1697897870] Cmd: ../llama.cpp/bin/main -m ../saved_models/llama-2-7b-q4_0.gguf --interactive-first --reverse-prompt . --presence-penalty 1.0 --frequency-penalty 1.0 --ctx-size 128 --n-predict 128
[1697897870] main: build = 1366 (8c70a5f)
[1697897870] main: built with Apple clang version 13.1.6 (clang-1316.0.21.2.3) for arm64-apple-darwin22.4.0
[1697897870] main: seed  = 1697897870
[1697897870] main: llama backend init
[1697897870] main: load the model and apply lora adapter, if any
[1697897870] warming up the model with an empty run
[1697897873] n_ctx: 128
[1697897873] 
[1697897873] system_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | 
[1697897873] add_bos: 1
[1697897873] tokenize the prompt
[1697897873] prompt: ""
[1697897873] tokens: [ '':1 ]
[1697897873] recalculate the cached logits (check): embd_inp.empty() false, n_matching_session_tokens 0, embd_inp.size() 1, session_tokens.size() 0, embd_inp.size() 1
[1697897873] inp_pfx: [ '':1, ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Inst':2799, 'ruction':4080, ':':29901, '':13, '':13 ]
[1697897873] inp_sfx: [ ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Response':13291, ':':29901, '':13, '':13 ]
[1697897873] main: interactive mode on.
[1697897873] Reverse prompt: '.'
[1697897873] sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 1.000000, frequency_penalty = 1.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
[1697897873] generate: n_ctx = 128, n_batch = 512, n_predict = 128, n_keep = 0
[1697897873] 

[1697897873] == Running in interactive mode. ==
[1697897873]  - Press Ctrl+C to interject at any time.
[1697897873]  - Press Return to return control to LLaMa.
 - To return control without starting a new line, end your input with '/'.
 - If you want to submit another line, end your input with '\'.

[1697897873] embd_inp.size(): 1, n_consumed: 0
[1697897873] eval: [ '':1 ]
[1697897873] n_past = 1
[1697897873] embd_inp.size(): 1, n_consumed: 1
[1697897873] waiting for user input
